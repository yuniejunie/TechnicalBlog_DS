## 데이터 리니지 및 메타데이터 자동화 시작하기

이 프로젝트는 데이터베이스를 데이터 리니지와 각 테이블/데이터베이스의 의존성을 기초로 매핑하는 방법을 보여줍니다.

## 목차<a name="top"></a>
* [현 상황 및 비즈니스 문제](#Situation_and_Complications)
* [해결방안](#Solution)
* [한계점](#limitations)
* [유지보수 가이드](#maintenance)

### 현 상황 및 비즈니스 문제<a name="Situation_and_Complications"></a>

이 프로젝트는 고객의 팀이 최종 근원 출처까지 **데이터 추적이 가능**하도록 돕는 것을 목표로 합니다.
현 클라이언트의 데이터베이스는 여러개의 뷰를 가지고있는데, 이 뷰들이 또 다른 뷰를 참조하면서 이 또 다른 뷰들이 다른 뷰들을 참조하게되면서 여러모로 데이터베이스의 최종 출처까지의 데이터 추적가능성을 낮추고 있습니다.
게다가 이러한 데이터 추적을 모두 팀원들이 직접 하나하나 시행하고 있는데, 이는 매우 시간이 걸릴 뿐 아니라 실수를 일으킬 가능성도 높아지는 방법입니다.
또 다른 문제점은 이렇게 생성한 정보를 일반 사용자들에게 이해하기 쉽고 도움이 되는 적절한 방법으로 발표하는 방법이 필요합니다.
[Top↑](#top) 

### 해결방안<a name="Solution"></a>
앞서 제시한 문제점들에 대한 해결방안으로 저는 [그래프 이론](https://en.wikipedia.org/wiki/Graph_theory#Computer_science)을 선택했습니다.
요약하자면, 이 모델이 각 테이블 들을 각각의 관계에 따라 연결 시켜 줄 것입니다.
각 노드들은 테이블 혹은 뷰를 대표하고, 각 엣지들은 그들의 관계를 나타냅니다.
각 엣지에 표시되어있는 화살표 방향은 이 테이블이 어떤 방향으로 참조되는 지를 나타냅니다.
예를 들면, 테이블 B가 'SELECT col1, col2 FROM TableA'와 같은 SQL 문으로 생성되었다고 생각합니다. 이 경우에 만들어지는 그래프는 다음과 같습니다.

![Nodes and edge](https://github.com/yuniejunie/yuniejunie.github.io/tree/master/assets/nodes_edge.png)

이 과정이 각 데이터베이스에 있는 각 테이블에 반복되면, 노드 대 노드 연결들이 점점 추가되면서 "그래프"가 데이터 리니지를 보여줄 수 있을 것입니다.

![Nodes_to_node_iteration](https://github.com/yuniejunie/yuniejunie.github.io/tree/master/assets/data_lineage_principle.PNG)
 
위 이미지를 참고하면, B 테이블의 데이터 리니지는 B 테이블 주위로 이 과정이 두 번 반복된 결과라고 볼 수 있습니다.
Now we can say Table B is based off of Table A and it is a source of Table C.

If we plot all tables/views in this way, it would be helpful to understand the flow of data and to build the data lineage along with its flow.

As a result, the following graph shows how the tables are mapped visually. From here, "The graph" will refer the same graph shown below in the backend.  

####### Graph without legend should go in here

Data lineage model works with automated metadata ingestion model as well since they both use the graph based on the Graph theory.
It is written in **Python 3**.

[Top↑](#top) 

## 한계점<a name="limitations"></a>

There are several limitations existing in the current approach. 

1. Currently Column comments for VIEWS are not realized in codes due to Cloudera Impala/Hive's technical limitations. 
Column comments for views are only able to be added on CREATE statements for each views. 
    - Contacted Cloudera and opened a feature request for this. 
2. Other softwares in the industry might be easier to deploy
    - I built this application in-house, but there are several graph database softwares exist in the market(e.g. Neo4j). 
      The reasons why I ended up building this application from the scratch were the following:
        1. Customizable: My client uses Cloudera Impala/Hive and it has the technical limitations. 
        I found out that metadata from its source tables were not inherited down to the predecessors, which results in lots of empty metadata. 
        My client was not interested in using any 3rd party application to look up data lineage or metadata. 
        Therefore, I had to find a way of ingesting metadata back to their database directly and decided to build a script from the scratch.
3. It is possible that the data lineage is missing some connections.
    
    - This application is based on each view's DDL statement. 
    It works great if Team Phoenix has a full access to all DDLs of the DB that's being mapped.
    If we don't have a full access, it is impossible to map that relationship. 
    Therefore, there is a potential concern that this application is missing some relationship 
    such as a view in other DBs was built upon one of our tables/views while we don't have any access to their DBs.
    
    ![No_Access_DBs_Limitation](https://github.com/yuniejunie/Cgl_DataLineage/blob/master/readme_reference/no_access_views.PNG)

## 유지보수 가이드<a name="maintenance"></a>
This section provides helpful information for someone to do maintenance. 

1. **connector.py**: To set up the connection to Impala and to build a graph database.
    - class **connector()** 
        - **connector(self, imp_host, dbname)**: To establish the Impala connection and return its cursor.
        - **close_conn(self, cursor)**: To close cursor when the job done. 
        - **get_tables(self, cursor)**: To get the list of tables of the selected Database.
        - **get_columns(self, cursor, table)**: To get the list of columns of the selected table.
        - **get_create_statements(self, tables, cursor, DB, show_failures = False)**: To get the create statement to analyze/extract the data lineage. `show_failures = TRUE` can be used to return the list of tables not process at this step.     
        - **parse_sql(self, sql)**: To parse DDL statement by using `sqlparse`
        - **build_graph(self, cursor, DB, G = nx.DiGraph())**: To build a graph database. The default is to build it from the scratch. 
            You can build a new graph over other existing graph by using `G = [EXISTING_G]`. It will be the best to use when you want to combine the graph in Drona and Peanut.
        - **update_G(self, cursor, DB, G)**: To refine the graph by deleting any nodes not found in the current DBs.  

2. **table_attributes.py**: To add table attributes to each node in the graph database(G)
    - **get_table_properties(table, cursor)**: To get table properties from DBs. Its return type is Dict.
        Currently it tracks COMMENT, SOURCE, LOAD_FREQUENCY, SECURITY_CLASSIFICATION, and CONTACT_INFO.
    - **get_col_comments(table, cursor, G, DB)**: To get column comments from DBs. Its return type is Dict.
    - **generate_metadata_metrics(table_props)**: To return the progress in table property. 
        It accepts the outcome of `get_table_properties` as a parameter. Its return type is a dict.
    - **generate_column_comment_metrics(col_comments)**: To return the progress of column comments.
        It accepts the outcome of `get_col_comments` as a parameter. Its return type is a dict.
    - **get_meta_data(G, tables, DB, cursor)**: To return the table_attributes to add on each node.
        This function uses every other functions mentioned above to get the final outcome. 
        It also iterates to enable metadata inheritance between sources and target tables.
    - **prep(G, tables, DB, cursor)**: To prepare ingesting table attributes to nodes.
        Mostly it combines table attributes to input to each node and it also changes the format of each node names for the consistency.
    - **update_G_metadata(G, tables, DB, cursor, tables_lists)**: To add table attributes to each node in G.
        Its return is a list of unfinished/unprocessed tables and its type is a list.
    - **get_outward_cols(G, table)**:
    - **get_outward_tables(G, table)**: To return a list of tables that uses the selected table as a source.
    - **get_tables_from_source(G, DB)**: To retrieve tables that use the tables in the input database as a source.
        Its return type is a Pandas DataFrame, which can easily be converted to .csv format.
    - **get_inward_cols(G, table)**: 
    - **get_inward_tables(G, table)**: To return a list of tables this table is using as sources.
    - **get_inward_tables_from_source_DB(G, DB)**: To retrieve tables that tables in the input database uses as sources.
        Its return type is a Pandas DataFrame.
    - **get_tables_from_source_table(G, table)**: To retrieve tables that use the selected table as a source.
        Its return type is a Pandas DataFrame.
    - **get_inward_tables_from_source_table(G, table)**: To retrieve tables that the selected table uses as sources.
        Its return type is a Pandas DataFrame.
    - **get_col_comments_from_source(G, cursor, table, DB)**: To inherit column comments from its sources and the mirror table in other environments. (e.g. dev_internal_fair & qas_internal_fair & prd_internal_fair)
        Its return type is a dict. This function is being used in `get_meta_data()`.
    - **check_table_type(DB)**: To get the list of tables and views. It returns 2 outputs: a list of views, a list of tables.
    - **get_metadata_status(G, input_tpe, userinput)**: To check the metadata progress. input_type can be specified either 1 or 2.
        1 is for DB input and 2 is for table level input. It calculates its metadata progress in percentage.
        Its return type is a Pandas DataFrame.
        
